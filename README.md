# ğŸ§  Churn Insight â€” Engenharia de Dados

Pipeline de **Engenharia de Dados em Python** desenvolvido no contexto do **Hackathon ONE**, responsÃ¡vel por realizar o tratamento, padronizaÃ§Ã£o, modelagem e preparaÃ§Ã£o dos dados de churn de clientes de telecomunicaÃ§Ãµes.

Este repositÃ³rio contÃ©m a versÃ£o inicial do pipeline de dados, incluindo:

- IngestÃ£o de dados a partir de CSV pÃºblico
- PadronizaÃ§Ã£o de schema e valores categÃ³ricos
- Tratamento de dados inconsistentes e valores nulos
- Feature engineering orientado a churn
- PreparaÃ§Ã£o dos dados para Machine Learning
- ExportaÃ§Ã£o de dataset final limpo e modelado

---

## ğŸš€ Tecnologias Utilizadas

- Python 3
- Pandas
- NumPy
- Scikit-learn
- Matplotlib
- Seaborn

---

## ğŸ—‚ï¸ Fonte dos Dados

- Dataset pÃºblico: **Telco Customer Churn**
- Origem: arquivo CSV hospedado em repositÃ³rio GitHub
- IngestÃ£o realizada via `pandas.read_csv()`

Os dados utilizados nÃ£o contÃªm informaÃ§Ãµes sensÃ­veis diretas e sÃ£o tratados para uso analÃ­tico e preditivo.

---

## ğŸ—ï¸ Arquitetura do Projeto

    flowchart LR
    A[ğŸ“‚ Dataset PÃºblico<br/>Telco Customer Churn CSV]
    B[âš™ï¸ Pipeline de Engenharia de Dados<br/>Python / Pandas]
    B1[PadronizaÃ§Ã£o de Schema]
    B2[Tratamento de Nulos e Tipos]
    B3[Feature Engineering]
    B4[Dataset Final Tratado<br/>churn_telco_dados_tratados.csv]

    C[ğŸ“Š Power BI<br/>Dashboard AnalÃ­tico]
    C1[PÃ¡gina 1<br/>VisÃ£o Executiva de Churn]
    C2[PÃ¡gina 2<br/>AnÃ¡lise de Perfil e Comportamento]

    D[ğŸ§  Churn Insight API<br/>Java / Spring Boot]
    D1[Endpoint /api/predict]
    D2[Regras de NegÃ³cio / Modelo Base]
    D3[Swagger UI]

    E[ğŸ¤– EvoluÃ§Ã£o Futura<br/>Modelo de Machine Learning]

    A --> B
    B --> B1 --> B2 --> B3 --> B4
    B4 --> C
    B4 --> D
    D --> E

Este diagrama representa o fluxo completo do projeto, desde a ingestÃ£o dos dados atÃ© o consumo analÃ­tico e preditivo, seguindo boas prÃ¡ticas de engenharia de dados, governanÃ§a e separaÃ§Ã£o de responsabilidades.

---

ğŸ§¹ Etapas de Tratamento dos Dados
âœ”ï¸ PadronizaÃ§Ã£o

* NormalizaÃ§Ã£o de nomes de colunas (snake_case)

* RemoÃ§Ã£o de acentos e caracteres especiais

* PadronizaÃ§Ã£o de valores textuais (lowercase)

âœ”ï¸ Qualidade dos Dados

* ConversÃ£o explÃ­cita de colunas numÃ©ricas

* Tratamento de valores nulos

* RemoÃ§Ã£o de registros duplicados por chave natural

* ValidaÃ§Ã£o de tipos e consistÃªncia do schema

âœ”ï¸ Feature Engineering

* CriaÃ§Ã£o da variÃ¡vel ticket_medio

* CriaÃ§Ã£o da variÃ¡vel cliente_novo

* Mapeamento da variÃ¡vel target evasao (0 / 1)

---

ğŸ§  PreparaÃ§Ã£o para Machine Learning

O pipeline realiza a preparaÃ§Ã£o completa dos dados para modelagem, incluindo:

* SeparaÃ§Ã£o entre features e variÃ¡vel target

* IdentificaÃ§Ã£o automÃ¡tica de colunas numÃ©ricas e categÃ³ricas

* Pipeline de prÃ©-processamento com:

 * StandardScaler para variÃ¡veis numÃ©ricas

 * OneHotEncoder para variÃ¡veis categÃ³ricas

* Modelo baseline com LogisticRegression

Essa estrutura permite fÃ¡cil substituiÃ§Ã£o ou evoluÃ§Ã£o do modelo.

---

ğŸ“ SaÃ­da do Pipeline

O pipeline gera um dataset final tratado e modelado:

  churn_telco_dados_tratados.csv

Este arquivo estÃ¡ pronto para uso em:

* Modelos de Machine Learning

* Dashboards analÃ­ticos (Power BI)

* APIs de inferÃªncia

* Camadas Silver / Gold em arquiteturas de dados

---

ğŸ” GovernanÃ§a de Dados

O pipeline segue princÃ­pios fundamentais de governanÃ§a:

* PadronizaÃ§Ã£o consistente de schema

* Dados anonimizados para fins analÃ­ticos

* Pipeline reprodutÃ­vel e versionÃ¡vel

* SeparaÃ§Ã£o clara entre ingestÃ£o, transformaÃ§Ã£o e saÃ­da

* Preparado para integraÃ§Ã£o com ambientes corporativos

---

ğŸš€ PrÃ³ximos Passos

* IntegraÃ§Ã£o com Data Lake / Lakehouse

* Versionamento de dados

* Monitoramento de qualidade e consistÃªncia

* Deploy em ambiente cloud (Oci / Databricks)

* EvoluÃ§Ã£o para arquitetura Bronze / Silver / Gold


---

# ğŸ§  Churn Insight API

API em Java/Spring Boot desenvolvida para o **Hackathon ONE**, responsÃ¡vel por calcular a probabilidade de churn (cancelamento) com base nas informaÃ§Ãµes do cliente.

Este repositÃ³rio contÃ©m a primeira versÃ£o funcional da API, incluindo:

* Endpoint de previsÃ£o (`/api/predict`)
* Estrutura de DTOs
* Modelo de prediÃ§Ã£o mockado
* Regras iniciais de cÃ¡lculo no utilitÃ¡rio `PredictionUtils`
* Swagger UI para testes rÃ¡pidos
* Estrutura para geraÃ§Ã£o de dataset sintÃ©tico

---

## ğŸš€ Tecnologias Utilizadas

* Java 17
* Spring Boot 3
* Maven
* H2 Database (in-memory)
* MapStruct
* Lombok
* Swagger (Springdoc OpenAPI)
* Python (para geraÃ§Ã£o do dataset)

---

## ğŸ“Œ Estrutura do Projeto

```
src/main/java/com/churninsight/api
â”œâ”€â”€ controller        â†’ PredictionController
â”œâ”€â”€ dto               â†’ CustomerInputDto, PredictionResponseDto
â”œâ”€â”€ mapper            â†’ PredictionMapper
â”œâ”€â”€ model             â†’ PredictionModel
â”œâ”€â”€ service           â†’ PredictionService
â””â”€â”€ util              â†’ PredictionUtils

data/
â”œâ”€â”€ .gitkeep
â””â”€â”€ scripts/
    â”œâ”€â”€ call_dataset_churn.py
    â””â”€â”€ .gitkeep
```

Arquitetura simples, modular e preparada para evoluÃ§Ã£o pelo time.

---

## ğŸ”® Endpoint Principal

### **POST /api/predict**

Envia dados de um cliente e recebe uma previsÃ£o calculada com base nas regras mockadas.

---

### **Exemplo de JSON enviado**

```json
{
  "contractMonths": 12,
  "paymentDelays": 1,
  "monthlyUsage": 230.5,
  "planType": "PREMIUM"
}
```

### **Exemplo de resposta**

```json
{
  "id": 1,
  "prediction": "No Churn",
  "probability": 0.125
}
```

---

## ğŸ“Š Regras de CÃ¡lculo (MVP)

A lÃ³gica inicial estÃ¡ centralizada em `PredictionUtils` e serve como **baseline para o time ajustar e evoluir**.

Cada atributo contribui com pesos positivos ou negativos para um *score*, normalizado entre **0 e 1**.

Essa camada poderÃ¡ futuramente ser substituÃ­da por:

* Modelo real de Machine Learning
* IntegraÃ§Ã£o com Python
* MicroserviÃ§o dedicado Ã  prediÃ§Ã£o
* Ajustes manuais da equipe de Data Science

---

## ğŸ§ Dataset SintÃ©tico (para Data Science)

O projeto inclui uma estrutura destinada Ã  geraÃ§Ã£o de dataset fictÃ­cio com **10.000 clientes**, usado para anÃ¡lises exploratÃ³rias (EDA), engenharia de atributos e modelagem supervisionada.

### ğŸ“ Estrutura

```
data/
â”œâ”€â”€ .gitkeep
â””â”€â”€ scripts/
    â”œâ”€â”€ call_dataset_churn.py
    â””â”€â”€ .gitkeep
```

O CSV gerado **nÃ£o Ã© versionado**, para evitar arquivos grandes no repositÃ³rio.

---

### ğŸ“„ Sobre o Script `call_dataset_churn.py`

O script gera dados sintÃ©ticos contendo:

**Dados do cliente**

* `name`, `age`, `city`, `state`
* `signup_date`

**Comportamento**

* `monthly_usage`
* `logins_per_month`
* `device_type`
* `plan_type`

**Pagamentos**

* `payment_delays`
* `late_payments_last_6m`
* `on_time_payment_ratio`

**Atributos para ML**

* `churn` (0 ou 1)
* `churn_probability` (score calculado)

---

### â–¶ï¸ Como gerar o dataset

No terminal:

```bash
cd data/scripts
python call_dataset_churn.py
```

O arquivo serÃ¡ gerado automaticamente em:

```
data/churn_customers_dataset.csv
```

---

## ğŸ”§ Como Executar a API

```bash
mvn spring-boot:run
```

A API estarÃ¡ disponÃ­vel em:

```
http://localhost:8080
```

### Swagger UI

```
http://localhost:8080/swagger-ui.html
```

---

## ğŸ§ª Testando com Postman / Insomnia

1. Crie uma requisiÃ§Ã£o **POST**
2. Use o endpoint:

```
http://localhost:8080/api/predict
```

3. Envie o JSON de exemplo.

---

## ğŸ¤ ContribuiÃ§Ã£o do Time

Este projeto foi iniciado para o Hackathon ONE e serÃ¡ evoluÃ­do em equipe.

ContribuiÃ§Ãµes bem-vindas:

* Ajustes nas regras de churn
* EvoluÃ§Ã£o do modelo de prediÃ§Ã£o
* RefatoraÃ§Ãµes de arquitetura
* CriaÃ§Ã£o de testes automatizados
* ExpansÃ£o do dataset
* ImplementaÃ§Ã£o de ML real

---
